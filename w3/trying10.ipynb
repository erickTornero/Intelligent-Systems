{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('row_cleaned.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1974 teenager martha moxley maggie grace move...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok so really like kris kristofferson usual eas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spoiler do not read this if you think about wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all people who have seen this wonderful...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recently bought dvd forgetting just how much ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review  sentiment\n",
       "Id                                                              \n",
       "0    1974 teenager martha moxley maggie grace move...          1\n",
       "1   ok so really like kris kristofferson usual eas...          0\n",
       "2   spoiler do not read this if you think about wa...          0\n",
       "3   hi for all people who have seen this wonderful...          1\n",
       "4    recently bought dvd forgetting just how much ...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts a text to a sequence of words.\n",
    "def review_wordlist(review, remove_stopwords=False):\n",
    "    # 1. Removing html tags\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This function splits a review into sentences\n",
    "def review_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # 1. Using nltk tokenizer\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    # 2. Loop for each sentence\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(review_wordlist(raw_sentence,\\\n",
    "                                            remove_stopwords))\n",
    "\n",
    "    # This returns the list of lists\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['review'].values\n",
    "Y = df['sentiment'].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'review': X_train, 'sentiment':Y_train})\n",
    "test = pd.DataFrame({'review': X_test, 'sentiment':Y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im large scarred heterosexual male ex bouncer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>watched this movie about six years ago recent...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obviously it seems many people really enjoyed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whats happening rgv he seems repeat himself ev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>have seen poor movies time but this really ta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  im large scarred heterosexual male ex bouncer ...          1\n",
       "1   watched this movie about six years ago recent...          0\n",
       "2  obviously it seems many people really enjoyed ...          0\n",
       "3  whats happening rgv he seems repeat himself ev...          0\n",
       "4   have seen poor movies time but this really ta...          0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/erick/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "\n",
    "sentences = []\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Importing the built-in logging module\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-08 14:29:06,993 : INFO : collecting all words and their counts\n",
      "2018-11-08 14:29:06,993 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-08 14:29:07,297 : INFO : PROGRESS: at sentence #10000, processed 1740785 words, keeping 59796 word types\n",
      "2018-11-08 14:29:07,585 : INFO : PROGRESS: at sentence #20000, processed 3455216 words, keeping 83128 word types\n",
      "2018-11-08 14:29:07,882 : INFO : PROGRESS: at sentence #30000, processed 5202230 words, keeping 101327 word types\n",
      "2018-11-08 14:29:08,031 : INFO : collected 109191 word types from a corpus of 6054816 raw words and 35000 sentences\n",
      "2018-11-08 14:29:08,032 : INFO : Loading a fresh vocabulary\n",
      "2018-11-08 14:29:08,086 : INFO : effective_min_count=40 retains 10220 unique words (9% of original 109191, drops 98971)\n",
      "2018-11-08 14:29:08,087 : INFO : effective_min_count=40 leaves 5600211 word corpus (92% of original 6054816, drops 454605)\n",
      "2018-11-08 14:29:08,113 : INFO : deleting the raw counts dictionary of 109191 items\n",
      "2018-11-08 14:29:08,117 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2018-11-08 14:29:08,117 : INFO : downsampling leaves estimated 4847355 word corpus (86.6% of prior 5600211)\n",
      "2018-11-08 14:29:08,152 : INFO : estimated required memory for 10220 words and 300 dimensions: 29638000 bytes\n",
      "2018-11-08 14:29:08,152 : INFO : resetting layer weights\n",
      "2018-11-08 14:29:08,267 : INFO : training model with 4 workers on 10220 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-11-08 14:29:09,274 : INFO : EPOCH 1 - PROGRESS: at 20.64% examples, 1000960 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:10,275 : INFO : EPOCH 1 - PROGRESS: at 44.32% examples, 1073148 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:11,278 : INFO : EPOCH 1 - PROGRESS: at 64.70% examples, 1043015 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:12,278 : INFO : EPOCH 1 - PROGRESS: at 83.57% examples, 1013133 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:13,028 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 14:29:13,032 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 14:29:13,039 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 14:29:13,043 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 14:29:13,044 : INFO : EPOCH - 1 : training on 6054816 raw words (4847211 effective words) took 4.8s, 1015522 effective words/s\n",
      "2018-11-08 14:29:14,054 : INFO : EPOCH 2 - PROGRESS: at 18.72% examples, 905805 words/s, in_qsize 8, out_qsize 0\n",
      "2018-11-08 14:29:15,060 : INFO : EPOCH 2 - PROGRESS: at 38.43% examples, 927934 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:16,065 : INFO : EPOCH 2 - PROGRESS: at 62.61% examples, 1006032 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:17,073 : INFO : EPOCH 2 - PROGRESS: at 82.25% examples, 993328 words/s, in_qsize 6, out_qsize 1\n",
      "2018-11-08 14:29:17,958 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 14:29:17,962 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 14:29:17,963 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 14:29:17,971 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 14:29:17,972 : INFO : EPOCH - 2 : training on 6054816 raw words (4846526 effective words) took 4.9s, 984832 effective words/s\n",
      "2018-11-08 14:29:18,981 : INFO : EPOCH 3 - PROGRESS: at 19.99% examples, 970763 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:19,991 : INFO : EPOCH 3 - PROGRESS: at 43.47% examples, 1049293 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:21,000 : INFO : EPOCH 3 - PROGRESS: at 67.02% examples, 1077311 words/s, in_qsize 8, out_qsize 0\n",
      "2018-11-08 14:29:22,005 : INFO : EPOCH 3 - PROGRESS: at 85.66% examples, 1034138 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:22,639 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 14:29:22,646 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 14:29:22,648 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 14:29:22,655 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 14:29:22,656 : INFO : EPOCH - 3 : training on 6054816 raw words (4847340 effective words) took 4.7s, 1036634 effective words/s\n",
      "2018-11-08 14:29:23,665 : INFO : EPOCH 4 - PROGRESS: at 23.05% examples, 1119879 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:24,672 : INFO : EPOCH 4 - PROGRESS: at 45.97% examples, 1109760 words/s, in_qsize 5, out_qsize 2\n",
      "2018-11-08 14:29:25,677 : INFO : EPOCH 4 - PROGRESS: at 66.57% examples, 1072280 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:26,683 : INFO : EPOCH 4 - PROGRESS: at 90.49% examples, 1092928 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:27,132 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 14:29:27,143 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 14:29:27,147 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 14:29:27,150 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 14:29:27,152 : INFO : EPOCH - 4 : training on 6054816 raw words (4848125 effective words) took 4.5s, 1080197 effective words/s\n",
      "2018-11-08 14:29:28,159 : INFO : EPOCH 5 - PROGRESS: at 22.21% examples, 1081781 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:29,166 : INFO : EPOCH 5 - PROGRESS: at 45.17% examples, 1090368 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:30,166 : INFO : EPOCH 5 - PROGRESS: at 68.51% examples, 1105343 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:31,170 : INFO : EPOCH 5 - PROGRESS: at 91.83% examples, 1110313 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 14:29:31,495 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 14:29:31,507 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 14:29:31,508 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 14:29:31,520 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 14:29:31,521 : INFO : EPOCH - 5 : training on 6054816 raw words (4847264 effective words) took 4.4s, 1111095 effective words/s\n",
      "2018-11-08 14:29:31,522 : INFO : training on a 30274080 raw words (24236466 effective words) took 23.3s, 1042227 effective words/s\n",
      "2018-11-08 14:29:31,522 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-11-08 14:29:31,613 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-11-08 14:29:31,614 : INFO : not storing attribute vectors_norm\n",
      "2018-11-08 14:29:31,614 : INFO : not storing attribute cum_table\n",
      "2018-11-08 14:29:31,832 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model....\")\n",
    "model = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "\n",
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.621584415435791),\n",
       " ('mans', 0.6111232042312622),\n",
       " ('lady', 0.514820396900177),\n",
       " ('boy', 0.4838256239891052),\n",
       " ('guy', 0.4823318421840668),\n",
       " ('men', 0.4685211181640625),\n",
       " ('soldier', 0.4443958103656769),\n",
       " ('lad', 0.43793267011642456),\n",
       " ('priest', 0.43287044763565063),\n",
       " ('lawyer', 0.4299907684326172)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.8090012669563293),\n",
       " ('dreadful', 0.7749406099319458),\n",
       " ('horrible', 0.774091362953186),\n",
       " ('atrocious', 0.7458703517913818),\n",
       " ('horrendous', 0.7247278690338135),\n",
       " ('abysmal', 0.7203051447868347),\n",
       " ('lousy', 0.7183014154434204),\n",
       " ('horrid', 0.7174729108810425),\n",
       " ('appalling', 0.6920179128646851),\n",
       " ('bad', 0.6690243482589722)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('awful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12826, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/erick/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 50000\n",
      "Review 2000 of 50000\n",
      "Review 3000 of 50000\n",
      "Review 4000 of 50000\n",
      "Review 5000 of 50000\n",
      "Review 6000 of 50000\n",
      "Review 7000 of 50000\n",
      "Review 8000 of 50000\n",
      "Review 9000 of 50000\n",
      "Review 10000 of 50000\n",
      "Review 11000 of 50000\n",
      "Review 12000 of 50000\n",
      "Review 13000 of 50000\n",
      "Review 14000 of 50000\n",
      "Review 15000 of 50000\n",
      "Review 16000 of 50000\n",
      "Review 17000 of 50000\n",
      "Review 18000 of 50000\n",
      "Review 19000 of 50000\n",
      "Review 20000 of 50000\n",
      "Review 21000 of 50000\n",
      "Review 22000 of 50000\n",
      "Review 23000 of 50000\n",
      "Review 24000 of 50000\n",
      "Review 25000 of 50000\n",
      "Review 26000 of 50000\n",
      "Review 27000 of 50000\n",
      "Review 28000 of 50000\n",
      "Review 29000 of 50000\n",
      "Review 30000 of 50000\n",
      "Review 31000 of 50000\n",
      "Review 32000 of 50000\n",
      "Review 33000 of 50000\n",
      "Review 34000 of 50000\n",
      "Review 35000 of 50000\n",
      "Review 36000 of 50000\n",
      "Review 37000 of 50000\n",
      "Review 38000 of 50000\n",
      "Review 39000 of 50000\n",
      "Review 40000 of 50000\n",
      "Review 41000 of 50000\n",
      "Review 42000 of 50000\n",
      "Review 43000 of 50000\n",
      "Review 44000 of 50000\n",
      "Review 45000 of 50000\n",
      "Review 46000 of 50000\n",
      "Review 47000 of 50000\n",
      "Review 48000 of 50000\n",
      "Review 49000 of 50000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Calculating average feature vector for training set\n",
    "clean_train_reviews = []\n",
    "for review in df['review']:\n",
    "    clean_train_reviews.append(review_wordlist(review, remove_stopwords=True))\n",
    "    \n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest to training data....\n"
     ]
    }
   ],
   "source": [
    "# Fitting a random forest classifier to the training data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "    \n",
    "print(\"Fitting random forest to training data....\")    \n",
    "forest = forest.fit(trainDataVecs, df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = forest.predict(trainDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ = Y_.reshape(Y_.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.reshape(result.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y1, y2):\n",
    "    le = y1.shape[0]\n",
    "    if le == y2.shape[0]:\n",
    "        er = y1 - y2\n",
    "        er = er*er\n",
    "        toter = np.sum(er)\n",
    "        return (le - toter)/le\n",
    "    else:\n",
    "        print('Input must be the same dimenssion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
