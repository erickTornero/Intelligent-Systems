{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('row_cleaned.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1974 teenager martha moxley maggie grace move...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok so really like kris kristofferson usual eas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spoiler do not read this if you think about wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all people who have seen this wonderful...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recently bought dvd forgetting just how much ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review  sentiment\n",
       "Id                                                              \n",
       "0    1974 teenager martha moxley maggie grace move...          1\n",
       "1   ok so really like kris kristofferson usual eas...          0\n",
       "2   spoiler do not read this if you think about wa...          0\n",
       "3   hi for all people who have seen this wonderful...          1\n",
       "4    recently bought dvd forgetting just how much ...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts a text to a sequence of words.\n",
    "def review_wordlist(review, remove_stopwords=False):\n",
    "    # 1. Removing html tags\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/erick/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This function splits a review into sentences\n",
    "def review_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # 1. Using nltk tokenizer\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    # 2. Loop for each sentence\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(review_wordlist(raw_sentence,\\\n",
    "                                            remove_stopwords))\n",
    "\n",
    "    # This returns the list of lists\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['review'].values\n",
    "Y = df['sentiment'].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'review': X_train, 'sentiment':Y_train})\n",
    "test = pd.DataFrame({'review': X_test, 'sentiment':Y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im large scarred heterosexual male ex bouncer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>watched this movie about six years ago recent...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obviously it seems many people really enjoyed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whats happening rgv he seems repeat himself ev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>have seen poor movies time but this really ta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  im large scarred heterosexual male ex bouncer ...          1\n",
       "1   watched this movie about six years ago recent...          0\n",
       "2  obviously it seems many people really enjoyed ...          0\n",
       "3  whats happening rgv he seems repeat himself ev...          0\n",
       "4   have seen poor movies time but this really ta...          0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/erick/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "\n",
    "sentences = []\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Importing the built-in logging module\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-08 16:39:41,724 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2018-11-08 16:39:41,739 : INFO : collecting all words and their counts\n",
      "2018-11-08 16:39:41,742 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-08 16:39:42,275 : INFO : PROGRESS: at sentence #10000, processed 1740785 words, keeping 59796 word types\n",
      "2018-11-08 16:39:42,649 : INFO : PROGRESS: at sentence #20000, processed 3455216 words, keeping 83128 word types\n",
      "2018-11-08 16:39:43,020 : INFO : PROGRESS: at sentence #30000, processed 5202230 words, keeping 101327 word types\n",
      "2018-11-08 16:39:43,197 : INFO : collected 109191 word types from a corpus of 6054816 raw words and 35000 sentences\n",
      "2018-11-08 16:39:43,198 : INFO : Loading a fresh vocabulary\n",
      "2018-11-08 16:39:43,475 : INFO : effective_min_count=40 retains 10220 unique words (9% of original 109191, drops 98971)\n",
      "2018-11-08 16:39:43,475 : INFO : effective_min_count=40 leaves 5600211 word corpus (92% of original 6054816, drops 454605)\n",
      "2018-11-08 16:39:43,510 : INFO : deleting the raw counts dictionary of 109191 items\n",
      "2018-11-08 16:39:43,514 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2018-11-08 16:39:43,514 : INFO : downsampling leaves estimated 4847355 word corpus (86.6% of prior 5600211)\n",
      "2018-11-08 16:39:43,546 : INFO : estimated required memory for 10220 words and 300 dimensions: 29638000 bytes\n",
      "2018-11-08 16:39:43,547 : INFO : resetting layer weights\n",
      "2018-11-08 16:39:43,693 : INFO : training model with 4 workers on 10220 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-11-08 16:39:44,701 : INFO : EPOCH 1 - PROGRESS: at 10.25% examples, 495939 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:39:45,717 : INFO : EPOCH 1 - PROGRESS: at 22.21% examples, 535926 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:39:46,727 : INFO : EPOCH 1 - PROGRESS: at 36.25% examples, 581199 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:39:47,730 : INFO : EPOCH 1 - PROGRESS: at 50.49% examples, 606893 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:39:48,733 : INFO : EPOCH 1 - PROGRESS: at 64.18% examples, 617672 words/s, in_qsize 8, out_qsize 0\n",
      "2018-11-08 16:39:49,740 : INFO : EPOCH 1 - PROGRESS: at 77.96% examples, 625919 words/s, in_qsize 6, out_qsize 1\n",
      "2018-11-08 16:39:50,756 : INFO : EPOCH 1 - PROGRESS: at 90.67% examples, 623107 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:39:51,322 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 16:39:51,340 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 16:39:51,344 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 16:39:51,346 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 16:39:51,347 : INFO : EPOCH - 1 : training on 6054816 raw words (4846598 effective words) took 7.7s, 633487 effective words/s\n",
      "2018-11-08 16:39:52,373 : INFO : EPOCH 2 - PROGRESS: at 15.14% examples, 720217 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:39:53,384 : INFO : EPOCH 2 - PROGRESS: at 30.45% examples, 730609 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:39:54,393 : INFO : EPOCH 2 - PROGRESS: at 44.50% examples, 709583 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:39:55,401 : INFO : EPOCH 2 - PROGRESS: at 58.73% examples, 702070 words/s, in_qsize 6, out_qsize 1\n",
      "2018-11-08 16:39:56,403 : INFO : EPOCH 2 - PROGRESS: at 73.55% examples, 706933 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:39:57,416 : INFO : EPOCH 2 - PROGRESS: at 89.08% examples, 713794 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:39:58,097 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 16:39:58,105 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 16:39:58,111 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 16:39:58,117 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 16:39:58,119 : INFO : EPOCH - 2 : training on 6054816 raw words (4847364 effective words) took 6.8s, 716290 effective words/s\n",
      "2018-11-08 16:39:59,125 : INFO : EPOCH 3 - PROGRESS: at 14.95% examples, 725996 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:00,137 : INFO : EPOCH 3 - PROGRESS: at 29.98% examples, 725607 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:01,151 : INFO : EPOCH 3 - PROGRESS: at 45.63% examples, 730574 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:02,159 : INFO : EPOCH 3 - PROGRESS: at 60.13% examples, 722192 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:03,162 : INFO : EPOCH 3 - PROGRESS: at 75.13% examples, 722817 words/s, in_qsize 8, out_qsize 0\n",
      "2018-11-08 16:40:04,174 : INFO : EPOCH 3 - PROGRESS: at 89.96% examples, 721808 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:04,850 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 16:40:04,854 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 16:40:04,859 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 16:40:04,871 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 16:40:04,871 : INFO : EPOCH - 3 : training on 6054816 raw words (4847049 effective words) took 6.7s, 718279 effective words/s\n",
      "2018-11-08 16:40:05,884 : INFO : EPOCH 4 - PROGRESS: at 12.99% examples, 621278 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:06,890 : INFO : EPOCH 4 - PROGRESS: at 25.29% examples, 612298 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:07,902 : INFO : EPOCH 4 - PROGRESS: at 39.18% examples, 629646 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:08,913 : INFO : EPOCH 4 - PROGRESS: at 52.05% examples, 626364 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:09,915 : INFO : EPOCH 4 - PROGRESS: at 64.15% examples, 617934 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:10,916 : INFO : EPOCH 4 - PROGRESS: at 79.51% examples, 639735 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:11,943 : INFO : EPOCH 4 - PROGRESS: at 94.82% examples, 650860 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:12,299 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 16:40:12,312 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 16:40:12,313 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 16:40:12,319 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 16:40:12,320 : INFO : EPOCH - 4 : training on 6054816 raw words (4847716 effective words) took 7.4s, 651399 effective words/s\n",
      "2018-11-08 16:40:13,336 : INFO : EPOCH 5 - PROGRESS: at 13.61% examples, 653178 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:14,358 : INFO : EPOCH 5 - PROGRESS: at 27.67% examples, 666259 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:15,366 : INFO : EPOCH 5 - PROGRESS: at 41.28% examples, 661180 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:16,366 : INFO : EPOCH 5 - PROGRESS: at 57.26% examples, 686908 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:17,384 : INFO : EPOCH 5 - PROGRESS: at 72.05% examples, 692463 words/s, in_qsize 8, out_qsize 0\n",
      "2018-11-08 16:40:18,399 : INFO : EPOCH 5 - PROGRESS: at 85.98% examples, 688597 words/s, in_qsize 7, out_qsize 0\n",
      "2018-11-08 16:40:19,255 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-08 16:40:19,269 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-08 16:40:19,273 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-08 16:40:19,277 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-08 16:40:19,277 : INFO : EPOCH - 5 : training on 6054816 raw words (4848333 effective words) took 6.9s, 697904 effective words/s\n",
      "2018-11-08 16:40:19,278 : INFO : training on a 30274080 raw words (24237060 effective words) took 35.6s, 681126 effective words/s\n",
      "2018-11-08 16:40:19,279 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-11-08 16:40:19,436 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-11-08 16:40:19,437 : INFO : not storing attribute vectors_norm\n",
      "2018-11-08 16:40:19,438 : INFO : not storing attribute cum_table\n",
      "2018-11-08 16:40:19,707 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model....\")\n",
    "model = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "\n",
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6021546125411987),\n",
       " ('mans', 0.5838137865066528),\n",
       " ('boy', 0.5711212158203125),\n",
       " ('lady', 0.5195755958557129),\n",
       " ('guy', 0.5087496638298035),\n",
       " ('himself', 0.5019849538803101),\n",
       " ('soldier', 0.48895955085754395),\n",
       " ('priest', 0.474894642829895),\n",
       " ('person', 0.45540571212768555),\n",
       " ('businessman', 0.4490589499473572)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.853543758392334),\n",
       " ('dreadful', 0.8195064067840576),\n",
       " ('horrible', 0.797717809677124),\n",
       " ('lousy', 0.7493232488632202),\n",
       " ('atrocious', 0.7393767833709717),\n",
       " ('horrendous', 0.7159844636917114),\n",
       " ('pathetic', 0.7141166925430298),\n",
       " ('bad', 0.6993353962898254),\n",
       " ('horrid', 0.6980143785476685),\n",
       " ('abysmal', 0.688806414604187)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('awful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10220, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/erick/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 35000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 35000\n",
      "Review 2000 of 35000\n",
      "Review 3000 of 35000\n",
      "Review 4000 of 35000\n",
      "Review 5000 of 35000\n",
      "Review 6000 of 35000\n",
      "Review 7000 of 35000\n",
      "Review 8000 of 35000\n",
      "Review 9000 of 35000\n",
      "Review 10000 of 35000\n",
      "Review 11000 of 35000\n",
      "Review 12000 of 35000\n",
      "Review 13000 of 35000\n",
      "Review 14000 of 35000\n",
      "Review 15000 of 35000\n",
      "Review 16000 of 35000\n",
      "Review 17000 of 35000\n",
      "Review 18000 of 35000\n",
      "Review 19000 of 35000\n",
      "Review 20000 of 35000\n",
      "Review 21000 of 35000\n",
      "Review 22000 of 35000\n",
      "Review 23000 of 35000\n",
      "Review 24000 of 35000\n",
      "Review 25000 of 35000\n",
      "Review 26000 of 35000\n",
      "Review 27000 of 35000\n",
      "Review 28000 of 35000\n",
      "Review 29000 of 35000\n",
      "Review 30000 of 35000\n",
      "Review 31000 of 35000\n",
      "Review 32000 of 35000\n",
      "Review 33000 of 35000\n",
      "Review 34000 of 35000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Calculating average feature vector for training set\n",
    "clean_train_reviews = []\n",
    "for review in train['review']:\n",
    "    clean_train_reviews.append(review_wordlist(review, remove_stopwords=True))\n",
    "    \n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/erick/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 15000\n",
      "Review 2000 of 15000\n",
      "Review 3000 of 15000\n",
      "Review 4000 of 15000\n",
      "Review 5000 of 15000\n",
      "Review 6000 of 15000\n",
      "Review 7000 of 15000\n",
      "Review 8000 of 15000\n",
      "Review 9000 of 15000\n",
      "Review 10000 of 15000\n",
      "Review 11000 of 15000\n",
      "Review 12000 of 15000\n",
      "Review 13000 of 15000\n",
      "Review 14000 of 15000\n"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vector for testing set\n",
    "clean_test_reviews = []\n",
    "for review in test['review']:\n",
    "    clean_test_reviews.append(review_wordlist(review, remove_stopwords=True))\n",
    "    \n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest to training data....\n"
     ]
    }
   ],
   "source": [
    "# Fitting a random forest classifier to the training data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "    \n",
    "print(\"Fitting random forest to training data....\")    \n",
    "forest = forest.fit(trainDataVecs, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = forest.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ = test['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ = Y_.reshape(Y_.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.reshape(result.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y1, y2):\n",
    "    le = y1.shape[0]\n",
    "    if le == y2.shape[0]:\n",
    "        er = y1 - y2\n",
    "        er = er*er\n",
    "        toter = np.sum(er)\n",
    "        return (le - toter)/le\n",
    "    else:\n",
    "        print('Input must be the same dimenssion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8451333333333333"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(result, Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tr = forest.predict(trainDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tr = train['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tr = Y_tr.reshape(Y_tr.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-7d60340553af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-6eb8bc386913>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(y1, y2)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mer\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtoter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtoter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score(result_tr, Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
